{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4calY-IqoXt"
      },
      "source": [
        "# Monte-Carlo Off-Police"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwJjLOm-qo5x"
      },
      "source": [
        "### Instalação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3H7uCS7cpC0y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt-get' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  \n",
        "!pip install gym[all]==00.25.1\n",
        "!pip install gym[atari,accept-rom-license]==00.25.1\n",
        "!pip install pyglet\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install optuna\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkBUSAK5qrJa",
        "outputId": "e13fc308-c3fd-433d-e6b1-6ab10530add3"
      },
      "outputs": [],
      "source": [
        "!mkdir log_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Usxq5Z1qwbj"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ala9DWUOqwsJ",
        "outputId": "ac44d2d3-a411-4024-a6e4-9869f2951f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorboard\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import gym\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdZ591gXq5aK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlWktrRj9GZ"
      },
      "source": [
        "### Para salvar vídeo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPyfQxD5z26J",
        "outputId": "3ed49373-2f2d-47f6-d865-68e531bb75e3"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "!pip install stable-baselines3[extra]\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "A gravação é feita com o wrapper [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_24l7Zn3P9mH"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record the given number of steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eFhFiESi4Y7"
      },
      "outputs": [],
      "source": [
        "# ideias adaptadas de : https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  html_code = f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "  return HTML(html_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHTzWX3krgKW"
      },
      "source": [
        "# Código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vU1HhpkMrJUX"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"Taxi-v3\"  \n",
        " \n",
        "env = gym.make(ENV_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGDXptJdP9mM"
      },
      "source": [
        "## Off-policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Off-Policy Padrão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0B2H4gpClZq0"
      },
      "outputs": [],
      "source": [
        "def choose_action(Q, state):\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "def choose_actionB(num_actions):\n",
        "    return np.random.randint(0, num_actions)\n",
        "\n",
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarloOffP(env, episodes, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "    \n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "    C = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "        \n",
        "        state = env.reset()\n",
        "    \n",
        "        # PARTE 1: executa um episódio completo\n",
        "        while done != True:   \n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios \n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "                \n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_actionB(num_actions)\n",
        "        \n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "            \n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "        \n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso \n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # PARTE 2: atualiza Q (e a política, implicitamente)\n",
        "        Gt = 0\n",
        "        W = 1\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "            C[s,a] = C[s,a] + W\n",
        "            delta = W * (Gt - Q[s,a])\n",
        "            Q[s,a] = Q[s,a] + (1/C[s,a])* delta\n",
        "            best = choose_action(Q,s)\n",
        "            if best != a: break\n",
        "            W = W*(1/(1/num_actions))\n",
        "\n",
        "    return sum_rewards_per_ep, Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Off-policy com weighted importance sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarloOffP_weighted(env, episodes, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "    \n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "    C = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "        \n",
        "        state = env.reset()\n",
        "    \n",
        "        # PARTE 1: executa um episódio completo\n",
        "        while done != True:   \n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios \n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "                \n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_actionB(num_actions)\n",
        "        \n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "            \n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "        \n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso \n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # PARTE 2: atualiza Q (e a política, implicitamente)\n",
        "        Gt = 0\n",
        "        W = 1\n",
        "        gamma_fact = 1\n",
        "        for (i, (s, a, r)) in enumerate(reversed(ep_trajectory)):\n",
        "            Gt = Gt + r\n",
        "            if i == 1 and gamma != 1:\n",
        "                gamma_fact *= (1-gamma)/gamma\n",
        "            else:\n",
        "                gamma_fact *= gamma\n",
        "            actual_w = W * gamma_fact\n",
        "            C[s,a] = C[s,a] + actual_w\n",
        "            delta = actual_w * (Gt - Q[s,a])\n",
        "            Q[s,a] = Q[s,a] + (1/C[s,a])* delta\n",
        "            best = choose_action(Q,s)\n",
        "            W = W*(1/(1/num_actions))\n",
        "            if best != a: break\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2yPwHrP9mN"
      },
      "source": [
        "## On-Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yXaGrkX_P9mN"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def pi_policy(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "\n",
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarloOnP(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "    \n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "        \n",
        "        state = env.reset()\n",
        "    \n",
        "        # PARTE 1: executa um episódio completo\n",
        "        while done != True:   \n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios \n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "                \n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = pi_policy(Q, state, num_actions, epsilon)\n",
        "        \n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "            \n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "        \n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso \n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # PARTE 2: atualiza Q (e a política, implicitamente)\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "            delta = Gt - Q[s,a]\n",
        "            Q[s,a] = Q[s,a] + lr * delta\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwSugu33P9mO"
      },
      "source": [
        "### Execução Off-Policy Padrão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au89jJmZTJFA"
      },
      "source": [
        "#### Otimiza Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bhgOjTT3_xfv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Arquivos de Programas\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from numpy.random.mtrand import gamma\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "\n",
        "ENV = gym.make(\"Taxi-v3\")\n",
        "\n",
        "\n",
        "# Esta função faz um treinamento com o Expected-SARSA, usando parâmetros sugeridos pelo Optuna.\n",
        "# Retorna a média dos retornos dos últimos 100 episódios.\n",
        "def train_values(trial : optuna.Trial):\n",
        "    \n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    gamma = trial.suggest_uniform('gamma', 0.02, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    #bins1 = trial.suggest_int('bins1', 5, 100)\n",
        "    #bins2 = trial.suggest_int('bins2', 5, 100)\n",
        "    \n",
        "    print(f\"\\nTRIAL #{trial.number}: eps={eps}, gamma={gamma}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    #env_wrapper = DiscreteObservationWrapper(ENV, [bins1,bins2])\n",
        "    (returns, _) = run_montecarloOffP(env, 20000, gamma, eps, render=False)\n",
        "    return sum(returns[-100:])/100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehjw4UTFA40G",
        "outputId": "b9c4cc5a-30fb-4aa8-94b1-a256ad3c5d20"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize', \n",
        "                            storage='sqlite:///optuna_studies.db', \n",
        "                            study_name= 'new2_MC_offpolice', \n",
        "                            load_if_exists=True)\n",
        "study.optimize(train_values, n_trials=20) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZEkS9SyTQxP"
      },
      "source": [
        "#### Execução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7I9paaU_CbTh"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e3Mc8g9pnRE",
        "outputId": "e2f1e190-4735-4911-b7a3-9cb1c257817a"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    r_max_plot = 10\n",
        "\n",
        "    EPISODES = 100000\n",
        "    LR = 0.01\n",
        "    GAMMA = 0.830525147061507\n",
        "    EPSILON = 0.05919712699520377\n",
        "\n",
        "    \n",
        "    # Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "    rewards, Qtable = run_montecarloOffP(env, EPISODES, GAMMA, EPSILON, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "    # Mostra um gráfico de episódios x retornos (não descontados)\n",
        "    # Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "    filename = f\"results/montecarloOffP-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "    plot_result(rewards, r_max_plot,100, 'offPolicyD')\n",
        "\n",
        "    # test_greedy_Q_policy(env, Qtable, 10, True)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execução Off-Policy(Weighted samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Otimiza parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import gamma\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "\n",
        "ENV = gym.make(\"Taxi-v3\")\n",
        "\n",
        "\n",
        "# Esta função faz um treinamento com o Expected-SARSA, usando parâmetros sugeridos pelo Optuna.\n",
        "# Retorna a média dos retornos dos últimos 100 episódios.\n",
        "def train_values(trial : optuna.Trial):\n",
        "    \n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    gamma = trial.suggest_uniform('gamma', 0.02, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    #bins1 = trial.suggest_int('bins1', 5, 100)\n",
        "    #bins2 = trial.suggest_int('bins2', 5, 100)\n",
        "    \n",
        "    print(f\"\\nTRIAL #{trial.number}: eps={eps}, gamma={gamma}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    #env_wrapper = DiscreteObservationWrapper(ENV, [bins1,bins2])\n",
        "    (returns, _) = run_montecarloOffP_weighted(ENV, 20000, gamma, eps, render=False)\n",
        "    return sum(returns[-100:])/100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize', \n",
        "                            storage='sqlite:///optuna_studies.db', \n",
        "                            study_name= 'new_MC_offpolice_weighted', \n",
        "                            load_if_exists=True)\n",
        "study.optimize(train_values, n_trials=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Execução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    r_max_plot = 10\n",
        "\n",
        "    EPISODES = 100000\n",
        "    LR = 0.01\n",
        "    GAMMA = 0.5723036890441425\n",
        "    EPSILON = 0.1915757619563071\n",
        "\n",
        "    \n",
        "    # Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "    rewards, Qtable = run_montecarloOffP_weighted(env, EPISODES, GAMMA, EPSILON, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "    # Mostra um gráfico de episódios x retornos (não descontados)\n",
        "    # Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "    filename = f\"results/montecarloOffP_weighted-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "    plot_result(rewards, r_max_plot,100, 'offPolicyD_weighted')\n",
        "\n",
        "    # test_greedy_Q_policy(env, Qtable, 10, True)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j8qwwqKP9mP"
      },
      "source": [
        "### Execução On-Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6U2CoAOP9mP"
      },
      "source": [
        "#### Otimiza Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wYzkdnvP9mP"
      },
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import gamma\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "\n",
        "ENV = gym.make(\"Taxi-v3\")\n",
        "\n",
        "\n",
        "# Esta função faz um treinamento com o Expected-SARSA, usando parâmetros sugeridos pelo Optuna.\n",
        "# Retorna a média dos retornos dos últimos 100 episódios.\n",
        "def train_values(trial : optuna.Trial):\n",
        "    \n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    lr = trial.suggest_uniform('learning_rate', 0.001, 1.0)\n",
        "    gamma = trial.suggest_uniform('gamma', 0.02, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "   \n",
        "    \n",
        "    print(f\"\\nTRIAL #{trial.number}: eps={eps}, gamma={gamma}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    \n",
        "    (returns, _) = run_montecarloOnP(ENV, 20000, lr, gamma, eps, render=False)\n",
        "    return sum(returns[-100:])/100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK5C5cAfP9mP",
        "outputId": "9b53579c-1b52-46ef-ac9d-c0f4a5467663"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize', \n",
        "                            storage='sqlite:///optuna_studies.db', \n",
        "                            study_name= 'new_MC_onpolice', \n",
        "                            load_if_exists=True)\n",
        "study.optimize(train_values, n_trials=20) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXQKv4x5P9mQ"
      },
      "source": [
        "#### Execução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcjKwJeKP9mQ",
        "outputId": "5c83fa0b-39e0-4f5f-f0c0-bb8f9cbb8aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100 Average Reward (last 100): -466.380\n",
            "Episode 200 Average Reward (last 100): -907.180\n",
            "Episode 300 Average Reward (last 100): -919.710\n",
            "Episode 400 Average Reward (last 100): -845.390\n",
            "Episode 500 Average Reward (last 100): -647.660\n",
            "Episode 600 Average Reward (last 100): -461.720\n",
            "Episode 700 Average Reward (last 100): -474.840\n",
            "Episode 800 Average Reward (last 100): -393.980\n",
            "Episode 900 Average Reward (last 100): -388.660\n",
            "Episode 1000 Average Reward (last 100): -319.070\n",
            "Episode 1100 Average Reward (last 100): -353.950\n",
            "Episode 1200 Average Reward (last 100): -324.430\n",
            "Episode 1300 Average Reward (last 100): -300.000\n",
            "Episode 1400 Average Reward (last 100): -284.720\n",
            "Episode 1500 Average Reward (last 100): -266.080\n",
            "Episode 1600 Average Reward (last 100): -252.130\n",
            "Episode 1700 Average Reward (last 100): -249.180\n",
            "Episode 1800 Average Reward (last 100): -228.040\n",
            "Episode 1900 Average Reward (last 100): -204.970\n",
            "Episode 2000 Average Reward (last 100): -182.050\n",
            "Episode 2100 Average Reward (last 100): -175.580\n",
            "Episode 2200 Average Reward (last 100): -120.950\n",
            "Episode 2300 Average Reward (last 100): -156.430\n",
            "Episode 2400 Average Reward (last 100): -101.210\n",
            "Episode 2500 Average Reward (last 100): -97.770\n",
            "Episode 2600 Average Reward (last 100): -101.920\n",
            "Episode 2700 Average Reward (last 100): -81.350\n",
            "Episode 2800 Average Reward (last 100): -69.390\n",
            "Episode 2900 Average Reward (last 100): -69.730\n",
            "Episode 3000 Average Reward (last 100): -49.480\n",
            "Episode 3100 Average Reward (last 100): -55.520\n",
            "Episode 3200 Average Reward (last 100): -56.500\n",
            "Episode 3300 Average Reward (last 100): -44.380\n",
            "Episode 3400 Average Reward (last 100): -45.890\n",
            "Episode 3500 Average Reward (last 100): -43.850\n",
            "Episode 3600 Average Reward (last 100): -29.380\n",
            "Episode 3700 Average Reward (last 100): -26.380\n",
            "Episode 3800 Average Reward (last 100): -50.400\n",
            "Episode 3900 Average Reward (last 100): -30.580\n",
            "Episode 4000 Average Reward (last 100): -34.000\n",
            "Episode 4100 Average Reward (last 100): -28.020\n",
            "Episode 4200 Average Reward (last 100): -17.950\n",
            "Episode 4300 Average Reward (last 100): -12.380\n",
            "Episode 4400 Average Reward (last 100): -15.850\n",
            "Episode 4500 Average Reward (last 100): -18.820\n",
            "Episode 4600 Average Reward (last 100): -23.280\n",
            "Episode 4700 Average Reward (last 100): -14.430\n",
            "Episode 4800 Average Reward (last 100): -20.300\n",
            "Episode 4900 Average Reward (last 100): -12.850\n",
            "Episode 5000 Average Reward (last 100): -11.060\n",
            "Episode 5100 Average Reward (last 100): -8.430\n",
            "Episode 5200 Average Reward (last 100): -11.640\n",
            "Episode 5300 Average Reward (last 100): -17.610\n",
            "Episode 5400 Average Reward (last 100): -13.120\n",
            "Episode 5500 Average Reward (last 100): -4.440\n",
            "Episode 5600 Average Reward (last 100): -10.920\n",
            "Episode 5700 Average Reward (last 100): -7.600\n",
            "Episode 5800 Average Reward (last 100): -8.320\n",
            "Episode 5900 Average Reward (last 100): -7.080\n",
            "Episode 6000 Average Reward (last 100): -9.980\n",
            "Episode 6100 Average Reward (last 100): -15.930\n",
            "Episode 6200 Average Reward (last 100): -4.100\n",
            "Episode 6300 Average Reward (last 100): -11.340\n",
            "Episode 6400 Average Reward (last 100): -9.510\n",
            "Episode 6500 Average Reward (last 100): -7.790\n",
            "Episode 6600 Average Reward (last 100): -10.080\n",
            "Episode 6700 Average Reward (last 100): -8.080\n",
            "Episode 6800 Average Reward (last 100): -5.250\n",
            "Episode 6900 Average Reward (last 100): -6.020\n",
            "Episode 7000 Average Reward (last 100): -10.490\n",
            "Episode 7100 Average Reward (last 100): -11.960\n",
            "Episode 7200 Average Reward (last 100): -6.940\n",
            "Episode 7300 Average Reward (last 100): -7.680\n",
            "Episode 7400 Average Reward (last 100): -5.330\n",
            "Episode 7500 Average Reward (last 100): -10.440\n",
            "Episode 7600 Average Reward (last 100): -8.370\n",
            "Episode 7700 Average Reward (last 100): -9.220\n",
            "Episode 7800 Average Reward (last 100): -8.570\n",
            "Episode 7900 Average Reward (last 100): -5.980\n",
            "Episode 8000 Average Reward (last 100): -4.880\n",
            "Episode 8100 Average Reward (last 100): -6.060\n",
            "Episode 8200 Average Reward (last 100): -4.680\n",
            "Episode 8300 Average Reward (last 100): -5.540\n",
            "Episode 8400 Average Reward (last 100): -4.920\n",
            "Episode 8500 Average Reward (last 100): -6.840\n",
            "Episode 8600 Average Reward (last 100): -4.680\n",
            "Episode 8700 Average Reward (last 100): -5.100\n",
            "Episode 8800 Average Reward (last 100): -7.080\n",
            "Episode 8900 Average Reward (last 100): -4.910\n",
            "Episode 9000 Average Reward (last 100): -4.720\n",
            "Episode 9100 Average Reward (last 100): -3.590\n",
            "Episode 9200 Average Reward (last 100): -5.460\n",
            "Episode 9300 Average Reward (last 100): -6.080\n",
            "Episode 9400 Average Reward (last 100): -7.670\n",
            "Episode 9500 Average Reward (last 100): -4.710\n",
            "Episode 9600 Average Reward (last 100): -7.690\n",
            "Episode 9700 Average Reward (last 100): -7.830\n",
            "Episode 9800 Average Reward (last 100): -7.090\n",
            "Episode 9900 Average Reward (last 100): -6.880\n",
            "Episode 10000 Average Reward (last 100): -4.590\n",
            "Episode 10100 Average Reward (last 100): -8.090\n",
            "Episode 10200 Average Reward (last 100): -6.480\n",
            "Episode 10300 Average Reward (last 100): -4.640\n",
            "Episode 10400 Average Reward (last 100): -3.920\n",
            "Episode 10500 Average Reward (last 100): -4.890\n",
            "Episode 10600 Average Reward (last 100): -6.940\n",
            "Episode 10700 Average Reward (last 100): -6.050\n",
            "Episode 10800 Average Reward (last 100): -5.630\n",
            "Episode 10900 Average Reward (last 100): -4.540\n",
            "Episode 11000 Average Reward (last 100): -5.880\n",
            "Episode 11100 Average Reward (last 100): -4.660\n",
            "Episode 11200 Average Reward (last 100): -4.620\n",
            "Episode 11300 Average Reward (last 100): -4.640\n",
            "Episode 11400 Average Reward (last 100): -7.080\n",
            "Episode 11500 Average Reward (last 100): -3.870\n",
            "Episode 11600 Average Reward (last 100): -6.090\n",
            "Episode 11700 Average Reward (last 100): -6.240\n",
            "Episode 11800 Average Reward (last 100): -5.160\n",
            "Episode 11900 Average Reward (last 100): -5.860\n",
            "Episode 12000 Average Reward (last 100): -5.150\n",
            "Episode 12100 Average Reward (last 100): -4.310\n",
            "Episode 12200 Average Reward (last 100): -5.920\n",
            "Episode 12300 Average Reward (last 100): -2.720\n",
            "Episode 12400 Average Reward (last 100): -3.750\n",
            "Episode 12500 Average Reward (last 100): -4.830\n",
            "Episode 12600 Average Reward (last 100): -6.390\n",
            "Episode 12700 Average Reward (last 100): -4.700\n",
            "Episode 12800 Average Reward (last 100): -4.000\n",
            "Episode 12900 Average Reward (last 100): -4.030\n",
            "Episode 13000 Average Reward (last 100): -4.110\n",
            "Episode 13100 Average Reward (last 100): -6.660\n",
            "Episode 13200 Average Reward (last 100): -3.700\n",
            "Episode 13300 Average Reward (last 100): -2.540\n",
            "Episode 13400 Average Reward (last 100): -5.010\n",
            "Episode 13500 Average Reward (last 100): -3.780\n",
            "Episode 13600 Average Reward (last 100): -5.230\n",
            "Episode 13700 Average Reward (last 100): -1.410\n",
            "Episode 13800 Average Reward (last 100): -4.650\n",
            "Episode 13900 Average Reward (last 100): -4.850\n",
            "Episode 14000 Average Reward (last 100): -5.740\n",
            "Episode 14100 Average Reward (last 100): -4.830\n",
            "Episode 14200 Average Reward (last 100): -3.980\n",
            "Episode 14300 Average Reward (last 100): -6.120\n",
            "Episode 14400 Average Reward (last 100): -4.480\n",
            "Episode 14500 Average Reward (last 100): -4.100\n",
            "Episode 14600 Average Reward (last 100): -3.570\n",
            "Episode 14700 Average Reward (last 100): -4.530\n",
            "Episode 14800 Average Reward (last 100): -4.600\n",
            "Episode 14900 Average Reward (last 100): -4.180\n",
            "Episode 15000 Average Reward (last 100): -5.920\n",
            "Episode 15100 Average Reward (last 100): -4.640\n",
            "Episode 15200 Average Reward (last 100): -5.080\n",
            "Episode 15300 Average Reward (last 100): -3.550\n",
            "Episode 15400 Average Reward (last 100): -4.510\n",
            "Episode 15500 Average Reward (last 100): -1.760\n",
            "Episode 15600 Average Reward (last 100): -2.450\n",
            "Episode 15700 Average Reward (last 100): -3.970\n",
            "Episode 15800 Average Reward (last 100): -5.050\n",
            "Episode 15900 Average Reward (last 100): -5.010\n",
            "Episode 16000 Average Reward (last 100): -3.460\n",
            "Episode 16100 Average Reward (last 100): -4.450\n",
            "Episode 16200 Average Reward (last 100): -4.400\n",
            "Episode 16300 Average Reward (last 100): -4.490\n",
            "Episode 16400 Average Reward (last 100): -6.920\n",
            "Episode 16500 Average Reward (last 100): -5.920\n",
            "Episode 16600 Average Reward (last 100): -2.960\n",
            "Episode 16700 Average Reward (last 100): -5.230\n",
            "Episode 16800 Average Reward (last 100): -5.830\n",
            "Episode 16900 Average Reward (last 100): -3.250\n",
            "Episode 17000 Average Reward (last 100): -4.470\n",
            "Episode 17100 Average Reward (last 100): -5.130\n",
            "Episode 17200 Average Reward (last 100): -5.140\n",
            "Episode 17300 Average Reward (last 100): -4.680\n",
            "Episode 17400 Average Reward (last 100): -2.570\n",
            "Episode 17500 Average Reward (last 100): -4.810\n",
            "Episode 17600 Average Reward (last 100): -6.590\n",
            "Episode 17700 Average Reward (last 100): -3.390\n",
            "Episode 17800 Average Reward (last 100): -5.280\n",
            "Episode 17900 Average Reward (last 100): -4.820\n",
            "Episode 18000 Average Reward (last 100): -3.820\n",
            "Episode 18100 Average Reward (last 100): -3.200\n",
            "Episode 18200 Average Reward (last 100): -5.380\n",
            "Episode 18300 Average Reward (last 100): -3.970\n",
            "Episode 18400 Average Reward (last 100): -4.070\n",
            "Episode 18500 Average Reward (last 100): -5.030\n",
            "Episode 18600 Average Reward (last 100): -4.830\n",
            "Episode 18700 Average Reward (last 100): -3.720\n",
            "Episode 18800 Average Reward (last 100): -4.170\n",
            "Episode 18900 Average Reward (last 100): -3.200\n",
            "Episode 19000 Average Reward (last 100): -4.540\n",
            "Episode 19100 Average Reward (last 100): -4.400\n",
            "Episode 19200 Average Reward (last 100): -3.750\n",
            "Episode 19300 Average Reward (last 100): -4.670\n",
            "Episode 19400 Average Reward (last 100): -3.790\n",
            "Episode 19500 Average Reward (last 100): -4.680\n",
            "Episode 19600 Average Reward (last 100): -1.900\n",
            "Episode 19700 Average Reward (last 100): -4.120\n",
            "Episode 19800 Average Reward (last 100): -3.920\n",
            "Episode 19900 Average Reward (last 100): -2.600\n",
            "Episode 20000 Average Reward (last 100): -2.580\n",
            "Episode 20100 Average Reward (last 100): -3.930\n",
            "Episode 20200 Average Reward (last 100): -4.720\n",
            "Episode 20300 Average Reward (last 100): -2.220\n",
            "Episode 20400 Average Reward (last 100): -5.070\n",
            "Episode 20500 Average Reward (last 100): -2.740\n",
            "Episode 20600 Average Reward (last 100): -4.430\n",
            "Episode 20700 Average Reward (last 100): -3.320\n",
            "Episode 20800 Average Reward (last 100): -2.280\n",
            "Episode 20900 Average Reward (last 100): -2.920\n",
            "Episode 21000 Average Reward (last 100): -4.260\n",
            "Episode 21100 Average Reward (last 100): -2.460\n",
            "Episode 21200 Average Reward (last 100): -4.400\n",
            "Episode 21300 Average Reward (last 100): -3.720\n",
            "Episode 21400 Average Reward (last 100): -4.250\n",
            "Episode 21500 Average Reward (last 100): -2.920\n",
            "Episode 21600 Average Reward (last 100): -6.440\n",
            "Episode 21700 Average Reward (last 100): -5.330\n",
            "Episode 21800 Average Reward (last 100): -3.460\n",
            "Episode 21900 Average Reward (last 100): -4.050\n",
            "Episode 22000 Average Reward (last 100): -5.160\n",
            "Episode 22100 Average Reward (last 100): -2.050\n",
            "Episode 22200 Average Reward (last 100): -2.590\n",
            "Episode 22300 Average Reward (last 100): -4.040\n",
            "Episode 22400 Average Reward (last 100): -4.090\n",
            "Episode 22500 Average Reward (last 100): -2.550\n",
            "Episode 22600 Average Reward (last 100): -3.840\n",
            "Episode 22700 Average Reward (last 100): -5.500\n",
            "Episode 22800 Average Reward (last 100): -4.890\n",
            "Episode 22900 Average Reward (last 100): -2.350\n",
            "Episode 23000 Average Reward (last 100): -3.230\n",
            "Episode 23100 Average Reward (last 100): -4.120\n",
            "Episode 23200 Average Reward (last 100): -2.820\n",
            "Episode 23300 Average Reward (last 100): -3.990\n",
            "Episode 23400 Average Reward (last 100): -5.250\n",
            "Episode 23500 Average Reward (last 100): -4.090\n",
            "Episode 23600 Average Reward (last 100): -4.420\n",
            "Episode 23700 Average Reward (last 100): -3.860\n",
            "Episode 23800 Average Reward (last 100): -4.170\n",
            "Episode 23900 Average Reward (last 100): -4.950\n",
            "Episode 24000 Average Reward (last 100): -2.600\n",
            "Episode 24100 Average Reward (last 100): -5.130\n",
            "Episode 24200 Average Reward (last 100): -4.990\n",
            "Episode 24300 Average Reward (last 100): -7.560\n",
            "Episode 24400 Average Reward (last 100): -4.380\n",
            "Episode 24500 Average Reward (last 100): -2.700\n",
            "Episode 24600 Average Reward (last 100): -3.820\n",
            "Episode 24700 Average Reward (last 100): 0.080\n",
            "Episode 24800 Average Reward (last 100): -4.090\n",
            "Episode 24900 Average Reward (last 100): -2.210\n",
            "Episode 25000 Average Reward (last 100): -5.520\n",
            "Episode 25100 Average Reward (last 100): -4.310\n",
            "Episode 25200 Average Reward (last 100): -6.330\n",
            "Episode 25300 Average Reward (last 100): -3.770\n",
            "Episode 25400 Average Reward (last 100): -2.260\n",
            "Episode 25500 Average Reward (last 100): -3.780\n",
            "Episode 25600 Average Reward (last 100): -3.680\n",
            "Episode 25700 Average Reward (last 100): -1.840\n",
            "Episode 25800 Average Reward (last 100): -3.570\n",
            "Episode 25900 Average Reward (last 100): -3.660\n",
            "Episode 26000 Average Reward (last 100): -2.960\n",
            "Episode 26100 Average Reward (last 100): -5.450\n",
            "Episode 26200 Average Reward (last 100): -1.380\n",
            "Episode 26300 Average Reward (last 100): -3.780\n",
            "Episode 26400 Average Reward (last 100): -3.540\n",
            "Episode 26500 Average Reward (last 100): -7.140\n",
            "Episode 26600 Average Reward (last 100): -5.160\n",
            "Episode 26700 Average Reward (last 100): -3.250\n",
            "Episode 26800 Average Reward (last 100): -3.130\n",
            "Episode 26900 Average Reward (last 100): -1.910\n",
            "Episode 27000 Average Reward (last 100): -5.170\n",
            "Episode 27100 Average Reward (last 100): -3.690\n",
            "Episode 27200 Average Reward (last 100): -3.440\n",
            "Episode 27300 Average Reward (last 100): -2.390\n",
            "Episode 27400 Average Reward (last 100): -3.130\n",
            "Episode 27500 Average Reward (last 100): -1.920\n",
            "Episode 27600 Average Reward (last 100): -3.620\n",
            "Episode 27700 Average Reward (last 100): -3.020\n",
            "Episode 27800 Average Reward (last 100): -4.970\n",
            "Episode 27900 Average Reward (last 100): -2.960\n",
            "Episode 28000 Average Reward (last 100): -3.030\n",
            "Episode 28100 Average Reward (last 100): -2.850\n",
            "Episode 28200 Average Reward (last 100): -5.500\n",
            "Episode 28300 Average Reward (last 100): -4.500\n",
            "Episode 28400 Average Reward (last 100): -0.830\n",
            "Episode 28500 Average Reward (last 100): -2.070\n",
            "Episode 28600 Average Reward (last 100): -4.480\n",
            "Episode 28700 Average Reward (last 100): -4.310\n",
            "Episode 28800 Average Reward (last 100): -2.640\n",
            "Episode 28900 Average Reward (last 100): -2.400\n",
            "Episode 29000 Average Reward (last 100): -3.600\n",
            "Episode 29100 Average Reward (last 100): -4.050\n",
            "Episode 29200 Average Reward (last 100): -2.140\n",
            "Episode 29300 Average Reward (last 100): -2.990\n",
            "Episode 29400 Average Reward (last 100): -3.940\n",
            "Episode 29500 Average Reward (last 100): -5.310\n",
            "Episode 29600 Average Reward (last 100): -4.790\n",
            "Episode 29700 Average Reward (last 100): -1.540\n",
            "Episode 29800 Average Reward (last 100): -2.670\n",
            "Episode 29900 Average Reward (last 100): -4.060\n",
            "Episode 30000 Average Reward (last 100): -4.790\n",
            "Episode 30100 Average Reward (last 100): -4.210\n",
            "Episode 30200 Average Reward (last 100): -3.500\n",
            "Episode 30300 Average Reward (last 100): -4.450\n",
            "Episode 30400 Average Reward (last 100): -2.780\n",
            "Episode 30500 Average Reward (last 100): -3.650\n",
            "Episode 30600 Average Reward (last 100): -4.010\n",
            "Episode 30700 Average Reward (last 100): -4.000\n",
            "Episode 30800 Average Reward (last 100): -3.330\n",
            "Episode 30900 Average Reward (last 100): -2.330\n",
            "Episode 31000 Average Reward (last 100): -3.030\n",
            "Episode 31100 Average Reward (last 100): -2.990\n",
            "Episode 31200 Average Reward (last 100): -3.750\n",
            "Episode 31300 Average Reward (last 100): -4.880\n",
            "Episode 31400 Average Reward (last 100): -2.950\n",
            "Episode 31500 Average Reward (last 100): -3.810\n",
            "Episode 31600 Average Reward (last 100): -4.820\n",
            "Episode 31700 Average Reward (last 100): -3.010\n",
            "Episode 31800 Average Reward (last 100): -2.160\n",
            "Episode 31900 Average Reward (last 100): -2.130\n",
            "Episode 32000 Average Reward (last 100): -3.460\n",
            "Episode 32100 Average Reward (last 100): -2.370\n",
            "Episode 32200 Average Reward (last 100): -4.610\n",
            "Episode 32300 Average Reward (last 100): -5.610\n",
            "Episode 32400 Average Reward (last 100): -4.080\n",
            "Episode 32500 Average Reward (last 100): -2.420\n",
            "Episode 32600 Average Reward (last 100): -5.490\n",
            "Episode 32700 Average Reward (last 100): -2.430\n",
            "Episode 32800 Average Reward (last 100): -3.030\n",
            "Episode 32900 Average Reward (last 100): -1.150\n",
            "Episode 33000 Average Reward (last 100): -3.400\n",
            "Episode 33100 Average Reward (last 100): -5.650\n",
            "Episode 33200 Average Reward (last 100): -4.120\n",
            "Episode 33300 Average Reward (last 100): -2.990\n",
            "Episode 33400 Average Reward (last 100): -2.930\n",
            "Episode 33500 Average Reward (last 100): -4.060\n",
            "Episode 33600 Average Reward (last 100): -3.380\n",
            "Episode 33700 Average Reward (last 100): -4.490\n",
            "Episode 33800 Average Reward (last 100): -2.240\n",
            "Episode 33900 Average Reward (last 100): -4.130\n",
            "Episode 34000 Average Reward (last 100): -1.870\n",
            "Episode 34100 Average Reward (last 100): -2.500\n",
            "Episode 34200 Average Reward (last 100): -3.740\n",
            "Episode 34300 Average Reward (last 100): -4.760\n",
            "Episode 34400 Average Reward (last 100): -4.330\n",
            "Episode 34500 Average Reward (last 100): -3.680\n",
            "Episode 34600 Average Reward (last 100): -2.730\n",
            "Episode 34700 Average Reward (last 100): -3.950\n",
            "Episode 34800 Average Reward (last 100): -2.960\n",
            "Episode 34900 Average Reward (last 100): -4.860\n",
            "Episode 35000 Average Reward (last 100): -1.790\n",
            "Episode 35100 Average Reward (last 100): -2.940\n",
            "Episode 35200 Average Reward (last 100): -3.800\n",
            "Episode 35300 Average Reward (last 100): -3.340\n",
            "Episode 35400 Average Reward (last 100): -5.150\n",
            "Episode 35500 Average Reward (last 100): -2.470\n",
            "Episode 35600 Average Reward (last 100): -2.890\n",
            "Episode 35700 Average Reward (last 100): -3.390\n",
            "Episode 35800 Average Reward (last 100): -3.460\n",
            "Episode 35900 Average Reward (last 100): -3.200\n",
            "Episode 36000 Average Reward (last 100): -2.120\n",
            "Episode 36100 Average Reward (last 100): -4.080\n",
            "Episode 36200 Average Reward (last 100): -5.900\n",
            "Episode 36300 Average Reward (last 100): -6.640\n",
            "Episode 36400 Average Reward (last 100): -4.550\n",
            "Episode 36500 Average Reward (last 100): -4.380\n",
            "Episode 36600 Average Reward (last 100): -1.110\n",
            "Episode 36700 Average Reward (last 100): -2.940\n",
            "Episode 36800 Average Reward (last 100): -2.310\n",
            "Episode 36900 Average Reward (last 100): -2.530\n",
            "Episode 37000 Average Reward (last 100): -4.280\n",
            "Episode 37100 Average Reward (last 100): -3.330\n",
            "Episode 37200 Average Reward (last 100): -2.630\n",
            "Episode 37300 Average Reward (last 100): -2.490\n",
            "Episode 37400 Average Reward (last 100): -4.630\n",
            "Episode 37500 Average Reward (last 100): -4.510\n",
            "Episode 37600 Average Reward (last 100): -3.780\n",
            "Episode 37700 Average Reward (last 100): -2.690\n",
            "Episode 37800 Average Reward (last 100): -2.060\n",
            "Episode 37900 Average Reward (last 100): -5.500\n",
            "Episode 38000 Average Reward (last 100): -4.490\n",
            "Episode 38100 Average Reward (last 100): -1.520\n",
            "Episode 38200 Average Reward (last 100): -2.900\n",
            "Episode 38300 Average Reward (last 100): -5.930\n",
            "Episode 38400 Average Reward (last 100): -5.220\n",
            "Episode 38500 Average Reward (last 100): -2.310\n",
            "Episode 38600 Average Reward (last 100): -2.290\n",
            "Episode 38700 Average Reward (last 100): -3.390\n",
            "Episode 38800 Average Reward (last 100): -2.610\n",
            "Episode 38900 Average Reward (last 100): -3.410\n",
            "Episode 39000 Average Reward (last 100): -1.010\n",
            "Episode 39100 Average Reward (last 100): -3.590\n",
            "Episode 39200 Average Reward (last 100): -3.990\n",
            "Episode 39300 Average Reward (last 100): -2.180\n",
            "Episode 39400 Average Reward (last 100): -3.900\n",
            "Episode 39500 Average Reward (last 100): -3.910\n",
            "Episode 39600 Average Reward (last 100): -1.330\n",
            "Episode 39700 Average Reward (last 100): -1.560\n",
            "Episode 39800 Average Reward (last 100): -2.750\n",
            "Episode 39900 Average Reward (last 100): -3.110\n",
            "Episode 40000 Average Reward (last 100): -4.730\n",
            "Episode 40100 Average Reward (last 100): -1.640\n",
            "Episode 40200 Average Reward (last 100): -1.940\n",
            "Episode 40300 Average Reward (last 100): -6.490\n",
            "Episode 40400 Average Reward (last 100): -6.060\n",
            "Episode 40500 Average Reward (last 100): -2.670\n",
            "Episode 40600 Average Reward (last 100): -2.550\n",
            "Episode 40700 Average Reward (last 100): -1.850\n",
            "Episode 40800 Average Reward (last 100): -3.590\n",
            "Episode 40900 Average Reward (last 100): -3.550\n",
            "Episode 41000 Average Reward (last 100): -3.680\n",
            "Episode 41100 Average Reward (last 100): -5.750\n",
            "Episode 41200 Average Reward (last 100): -4.450\n",
            "Episode 41300 Average Reward (last 100): -4.980\n",
            "Episode 41400 Average Reward (last 100): -2.320\n",
            "Episode 41500 Average Reward (last 100): -3.230\n",
            "Episode 41600 Average Reward (last 100): -3.740\n",
            "Episode 41700 Average Reward (last 100): -1.770\n",
            "Episode 41800 Average Reward (last 100): -3.790\n",
            "Episode 41900 Average Reward (last 100): -3.700\n",
            "Episode 42000 Average Reward (last 100): -2.830\n",
            "Episode 42100 Average Reward (last 100): -2.810\n",
            "Episode 42200 Average Reward (last 100): -2.130\n",
            "Episode 42300 Average Reward (last 100): -4.370\n",
            "Episode 42400 Average Reward (last 100): -4.090\n",
            "Episode 42500 Average Reward (last 100): -4.280\n",
            "Episode 42600 Average Reward (last 100): -5.780\n",
            "Episode 42700 Average Reward (last 100): -0.480\n",
            "Episode 42800 Average Reward (last 100): -4.680\n",
            "Episode 42900 Average Reward (last 100): -4.340\n",
            "Episode 43000 Average Reward (last 100): -5.940\n",
            "Episode 43100 Average Reward (last 100): -2.210\n",
            "Episode 43200 Average Reward (last 100): -1.950\n",
            "Episode 43300 Average Reward (last 100): -3.580\n",
            "Episode 43400 Average Reward (last 100): -2.470\n",
            "Episode 43500 Average Reward (last 100): -4.310\n",
            "Episode 43600 Average Reward (last 100): -3.210\n",
            "Episode 43700 Average Reward (last 100): -2.870\n",
            "Episode 43800 Average Reward (last 100): -2.520\n",
            "Episode 43900 Average Reward (last 100): -2.700\n",
            "Episode 44000 Average Reward (last 100): -4.800\n",
            "Episode 44100 Average Reward (last 100): -5.220\n",
            "Episode 44200 Average Reward (last 100): -2.670\n",
            "Episode 44300 Average Reward (last 100): -3.040\n",
            "Episode 44400 Average Reward (last 100): -3.720\n",
            "Episode 44500 Average Reward (last 100): -2.220\n",
            "Episode 44600 Average Reward (last 100): -3.800\n",
            "Episode 44700 Average Reward (last 100): -2.300\n",
            "Episode 44800 Average Reward (last 100): -1.530\n",
            "Episode 44900 Average Reward (last 100): -3.720\n",
            "Episode 45000 Average Reward (last 100): -5.000\n",
            "Episode 45100 Average Reward (last 100): -2.090\n",
            "Episode 45200 Average Reward (last 100): -1.900\n",
            "Episode 45300 Average Reward (last 100): -5.610\n",
            "Episode 45400 Average Reward (last 100): -3.020\n",
            "Episode 45500 Average Reward (last 100): -3.600\n",
            "Episode 45600 Average Reward (last 100): -3.870\n",
            "Episode 45700 Average Reward (last 100): -3.370\n",
            "Episode 45800 Average Reward (last 100): -3.620\n",
            "Episode 45900 Average Reward (last 100): -5.060\n",
            "Episode 46000 Average Reward (last 100): -5.460\n",
            "Episode 46100 Average Reward (last 100): -2.090\n",
            "Episode 46200 Average Reward (last 100): -3.560\n",
            "Episode 46300 Average Reward (last 100): -2.450\n",
            "Episode 46400 Average Reward (last 100): -2.980\n",
            "Episode 46500 Average Reward (last 100): -2.630\n",
            "Episode 46600 Average Reward (last 100): -3.660\n",
            "Episode 46700 Average Reward (last 100): -1.960\n",
            "Episode 46800 Average Reward (last 100): -2.230\n",
            "Episode 46900 Average Reward (last 100): -3.440\n",
            "Episode 47000 Average Reward (last 100): -3.610\n",
            "Episode 47100 Average Reward (last 100): -2.800\n",
            "Episode 47200 Average Reward (last 100): -4.240\n",
            "Episode 47300 Average Reward (last 100): -1.910\n",
            "Episode 47400 Average Reward (last 100): -3.600\n",
            "Episode 47500 Average Reward (last 100): -4.110\n",
            "Episode 47600 Average Reward (last 100): -1.360\n",
            "Episode 47700 Average Reward (last 100): -4.030\n",
            "Episode 47800 Average Reward (last 100): -6.210\n",
            "Episode 47900 Average Reward (last 100): -3.760\n",
            "Episode 48000 Average Reward (last 100): -3.990\n",
            "Episode 48100 Average Reward (last 100): -3.540\n",
            "Episode 48200 Average Reward (last 100): -4.840\n",
            "Episode 48300 Average Reward (last 100): -1.800\n",
            "Episode 48400 Average Reward (last 100): -3.000\n",
            "Episode 48500 Average Reward (last 100): -3.650\n",
            "Episode 48600 Average Reward (last 100): 0.020\n",
            "Episode 48700 Average Reward (last 100): -5.070\n",
            "Episode 48800 Average Reward (last 100): -1.600\n",
            "Episode 48900 Average Reward (last 100): -2.360\n",
            "Episode 49000 Average Reward (last 100): -4.490\n",
            "Episode 49100 Average Reward (last 100): -5.000\n",
            "Episode 49200 Average Reward (last 100): -4.860\n",
            "Episode 49300 Average Reward (last 100): -4.450\n",
            "Episode 49400 Average Reward (last 100): -4.870\n",
            "Episode 49500 Average Reward (last 100): -4.420\n",
            "Episode 49600 Average Reward (last 100): -2.580\n",
            "Episode 49700 Average Reward (last 100): -2.480\n",
            "Episode 49800 Average Reward (last 100): -4.720\n",
            "Episode 49900 Average Reward (last 100): -4.630\n",
            "Episode 50000 Average Reward (last 100): -4.280\n",
            "Episode 50100 Average Reward (last 100): -3.370\n",
            "Episode 50200 Average Reward (last 100): -2.800\n",
            "Episode 50300 Average Reward (last 100): -1.750\n",
            "Episode 50400 Average Reward (last 100): -2.600\n",
            "Episode 50500 Average Reward (last 100): -2.240\n",
            "Episode 50600 Average Reward (last 100): -4.450\n",
            "Episode 50700 Average Reward (last 100): -2.600\n",
            "Episode 50800 Average Reward (last 100): -1.620\n",
            "Episode 50900 Average Reward (last 100): -3.270\n",
            "Episode 51000 Average Reward (last 100): -4.360\n",
            "Episode 51100 Average Reward (last 100): -3.520\n",
            "Episode 51200 Average Reward (last 100): -1.550\n",
            "Episode 51300 Average Reward (last 100): -1.130\n",
            "Episode 51400 Average Reward (last 100): -4.050\n",
            "Episode 51500 Average Reward (last 100): -4.290\n",
            "Episode 51600 Average Reward (last 100): -2.870\n",
            "Episode 51700 Average Reward (last 100): -1.760\n",
            "Episode 51800 Average Reward (last 100): -3.370\n",
            "Episode 51900 Average Reward (last 100): -3.080\n",
            "Episode 52000 Average Reward (last 100): -5.240\n",
            "Episode 52100 Average Reward (last 100): -1.570\n",
            "Episode 52200 Average Reward (last 100): -4.840\n",
            "Episode 52300 Average Reward (last 100): -4.250\n",
            "Episode 52400 Average Reward (last 100): -2.720\n",
            "Episode 52500 Average Reward (last 100): -2.660\n",
            "Episode 52600 Average Reward (last 100): -4.190\n",
            "Episode 52700 Average Reward (last 100): -2.670\n",
            "Episode 52800 Average Reward (last 100): -2.110\n",
            "Episode 52900 Average Reward (last 100): -3.190\n",
            "Episode 53000 Average Reward (last 100): -3.650\n",
            "Episode 53100 Average Reward (last 100): -2.320\n",
            "Episode 53200 Average Reward (last 100): -2.960\n",
            "Episode 53300 Average Reward (last 100): -2.100\n",
            "Episode 53400 Average Reward (last 100): -4.480\n",
            "Episode 53500 Average Reward (last 100): -1.950\n",
            "Episode 53600 Average Reward (last 100): -2.270\n",
            "Episode 53700 Average Reward (last 100): -2.520\n",
            "Episode 53800 Average Reward (last 100): -2.260\n",
            "Episode 53900 Average Reward (last 100): -4.070\n",
            "Episode 54000 Average Reward (last 100): -3.710\n",
            "Episode 54100 Average Reward (last 100): -3.260\n",
            "Episode 54200 Average Reward (last 100): -2.450\n",
            "Episode 54300 Average Reward (last 100): -3.160\n",
            "Episode 54400 Average Reward (last 100): -0.860\n",
            "Episode 54500 Average Reward (last 100): -3.410\n",
            "Episode 54600 Average Reward (last 100): -2.410\n",
            "Episode 54700 Average Reward (last 100): -2.160\n",
            "Episode 54800 Average Reward (last 100): -1.860\n",
            "Episode 54900 Average Reward (last 100): -2.210\n",
            "Episode 55000 Average Reward (last 100): -0.650\n",
            "Episode 55100 Average Reward (last 100): -4.390\n",
            "Episode 55200 Average Reward (last 100): -5.030\n",
            "Episode 55300 Average Reward (last 100): -4.480\n",
            "Episode 55400 Average Reward (last 100): -2.270\n",
            "Episode 55500 Average Reward (last 100): -1.890\n",
            "Episode 55600 Average Reward (last 100): -3.120\n",
            "Episode 55700 Average Reward (last 100): -2.370\n",
            "Episode 55800 Average Reward (last 100): -1.180\n",
            "Episode 55900 Average Reward (last 100): 0.000\n",
            "Episode 56000 Average Reward (last 100): -1.570\n",
            "Episode 56100 Average Reward (last 100): -3.920\n",
            "Episode 56200 Average Reward (last 100): -2.950\n",
            "Episode 56300 Average Reward (last 100): -4.320\n",
            "Episode 56400 Average Reward (last 100): -1.530\n",
            "Episode 56500 Average Reward (last 100): -1.820\n",
            "Episode 56600 Average Reward (last 100): -2.090\n",
            "Episode 56700 Average Reward (last 100): -2.970\n",
            "Episode 56800 Average Reward (last 100): -3.130\n",
            "Episode 56900 Average Reward (last 100): -1.810\n",
            "Episode 57000 Average Reward (last 100): -2.070\n",
            "Episode 57100 Average Reward (last 100): -0.900\n",
            "Episode 57200 Average Reward (last 100): -2.080\n",
            "Episode 57300 Average Reward (last 100): -2.930\n",
            "Episode 57400 Average Reward (last 100): -3.210\n",
            "Episode 57500 Average Reward (last 100): -2.980\n",
            "Episode 57600 Average Reward (last 100): -6.990\n",
            "Episode 57700 Average Reward (last 100): -3.690\n",
            "Episode 57800 Average Reward (last 100): -3.010\n",
            "Episode 57900 Average Reward (last 100): -3.020\n",
            "Episode 58000 Average Reward (last 100): -2.020\n",
            "Episode 58100 Average Reward (last 100): -2.970\n",
            "Episode 58200 Average Reward (last 100): -2.310\n",
            "Episode 58300 Average Reward (last 100): -2.510\n",
            "Episode 58400 Average Reward (last 100): -1.650\n",
            "Episode 58500 Average Reward (last 100): -2.860\n",
            "Episode 58600 Average Reward (last 100): -4.110\n",
            "Episode 58700 Average Reward (last 100): -2.760\n",
            "Episode 58800 Average Reward (last 100): -3.020\n",
            "Episode 58900 Average Reward (last 100): -3.010\n",
            "Episode 59000 Average Reward (last 100): -0.680\n",
            "Episode 59100 Average Reward (last 100): -2.590\n",
            "Episode 59200 Average Reward (last 100): -2.840\n",
            "Episode 59300 Average Reward (last 100): -4.030\n",
            "Episode 59400 Average Reward (last 100): -3.120\n",
            "Episode 59500 Average Reward (last 100): -3.130\n",
            "Episode 59600 Average Reward (last 100): -1.450\n",
            "Episode 59700 Average Reward (last 100): -3.370\n",
            "Episode 59800 Average Reward (last 100): -3.490\n",
            "Episode 59900 Average Reward (last 100): -2.190\n",
            "Episode 60000 Average Reward (last 100): -3.680\n",
            "Episode 60100 Average Reward (last 100): -2.330\n",
            "Episode 60200 Average Reward (last 100): -2.800\n",
            "Episode 60300 Average Reward (last 100): -3.130\n",
            "Episode 60400 Average Reward (last 100): -3.670\n",
            "Episode 60500 Average Reward (last 100): -3.850\n",
            "Episode 60600 Average Reward (last 100): -4.690\n",
            "Episode 60700 Average Reward (last 100): -3.050\n",
            "Episode 60800 Average Reward (last 100): -3.320\n",
            "Episode 60900 Average Reward (last 100): -4.740\n",
            "Episode 61000 Average Reward (last 100): -5.050\n",
            "Episode 61100 Average Reward (last 100): 0.260\n",
            "Episode 61200 Average Reward (last 100): -3.530\n",
            "Episode 61300 Average Reward (last 100): -4.390\n",
            "Episode 61400 Average Reward (last 100): -3.220\n",
            "Episode 61500 Average Reward (last 100): -3.000\n",
            "Episode 61600 Average Reward (last 100): -2.950\n",
            "Episode 61700 Average Reward (last 100): -5.150\n",
            "Episode 61800 Average Reward (last 100): -2.580\n",
            "Episode 61900 Average Reward (last 100): -1.810\n",
            "Episode 62000 Average Reward (last 100): -4.670\n",
            "Episode 62100 Average Reward (last 100): -3.460\n",
            "Episode 62200 Average Reward (last 100): -2.180\n",
            "Episode 62300 Average Reward (last 100): -2.870\n",
            "Episode 62400 Average Reward (last 100): -1.280\n",
            "Episode 62500 Average Reward (last 100): -1.120\n",
            "Episode 62600 Average Reward (last 100): -2.750\n",
            "Episode 62700 Average Reward (last 100): -2.100\n",
            "Episode 62800 Average Reward (last 100): -5.270\n",
            "Episode 62900 Average Reward (last 100): -5.450\n",
            "Episode 63000 Average Reward (last 100): -3.320\n",
            "Episode 63100 Average Reward (last 100): -2.980\n",
            "Episode 63200 Average Reward (last 100): -2.960\n",
            "Episode 63300 Average Reward (last 100): -3.020\n",
            "Episode 63400 Average Reward (last 100): -2.190\n",
            "Episode 63500 Average Reward (last 100): -2.500\n",
            "Episode 63600 Average Reward (last 100): -2.350\n",
            "Episode 63700 Average Reward (last 100): -4.410\n",
            "Episode 63800 Average Reward (last 100): -2.040\n",
            "Episode 63900 Average Reward (last 100): -3.750\n",
            "Episode 64000 Average Reward (last 100): -2.810\n",
            "Episode 64100 Average Reward (last 100): -3.240\n",
            "Episode 64200 Average Reward (last 100): -2.360\n",
            "Episode 64300 Average Reward (last 100): -1.800\n",
            "Episode 64400 Average Reward (last 100): -2.840\n",
            "Episode 64500 Average Reward (last 100): -3.060\n",
            "Episode 64600 Average Reward (last 100): -3.130\n",
            "Episode 64700 Average Reward (last 100): -4.430\n",
            "Episode 64800 Average Reward (last 100): -2.760\n",
            "Episode 64900 Average Reward (last 100): -4.560\n",
            "Episode 65000 Average Reward (last 100): -5.110\n",
            "Episode 65100 Average Reward (last 100): -3.880\n",
            "Episode 65200 Average Reward (last 100): -4.300\n",
            "Episode 65300 Average Reward (last 100): -2.860\n",
            "Episode 65400 Average Reward (last 100): -4.910\n",
            "Episode 65500 Average Reward (last 100): -3.970\n",
            "Episode 65600 Average Reward (last 100): -4.090\n",
            "Episode 65700 Average Reward (last 100): -4.290\n",
            "Episode 65800 Average Reward (last 100): -2.520\n",
            "Episode 65900 Average Reward (last 100): -2.650\n",
            "Episode 66000 Average Reward (last 100): -4.550\n",
            "Episode 66100 Average Reward (last 100): -1.570\n",
            "Episode 66200 Average Reward (last 100): -2.620\n",
            "Episode 66300 Average Reward (last 100): -3.700\n",
            "Episode 66400 Average Reward (last 100): -2.990\n",
            "Episode 66500 Average Reward (last 100): -5.390\n",
            "Episode 66600 Average Reward (last 100): -3.490\n",
            "Episode 66700 Average Reward (last 100): -5.270\n",
            "Episode 66800 Average Reward (last 100): -2.160\n",
            "Episode 66900 Average Reward (last 100): -3.020\n",
            "Episode 67000 Average Reward (last 100): -2.440\n",
            "Episode 67100 Average Reward (last 100): -4.600\n",
            "Episode 67200 Average Reward (last 100): -3.080\n",
            "Episode 67300 Average Reward (last 100): -4.310\n",
            "Episode 67400 Average Reward (last 100): -4.660\n",
            "Episode 67500 Average Reward (last 100): -2.840\n",
            "Episode 67600 Average Reward (last 100): -3.440\n",
            "Episode 67700 Average Reward (last 100): -2.970\n",
            "Episode 67800 Average Reward (last 100): -2.050\n",
            "Episode 67900 Average Reward (last 100): -3.370\n",
            "Episode 68000 Average Reward (last 100): -5.050\n",
            "Episode 68100 Average Reward (last 100): -2.760\n",
            "Episode 68200 Average Reward (last 100): -0.650\n",
            "Episode 68300 Average Reward (last 100): -3.690\n",
            "Episode 68400 Average Reward (last 100): -4.730\n",
            "Episode 68500 Average Reward (last 100): -2.320\n",
            "Episode 68600 Average Reward (last 100): -3.600\n",
            "Episode 68700 Average Reward (last 100): -1.660\n",
            "Episode 68800 Average Reward (last 100): -3.280\n",
            "Episode 68900 Average Reward (last 100): -1.560\n",
            "Episode 69000 Average Reward (last 100): -3.570\n",
            "Episode 69100 Average Reward (last 100): -4.560\n",
            "Episode 69200 Average Reward (last 100): -3.620\n",
            "Episode 69300 Average Reward (last 100): -3.450\n",
            "Episode 69400 Average Reward (last 100): -2.850\n",
            "Episode 69500 Average Reward (last 100): -1.000\n",
            "Episode 69600 Average Reward (last 100): -2.770\n",
            "Episode 69700 Average Reward (last 100): -1.970\n",
            "Episode 69800 Average Reward (last 100): -1.010\n",
            "Episode 69900 Average Reward (last 100): -2.070\n",
            "Episode 70000 Average Reward (last 100): -2.510\n",
            "Episode 70100 Average Reward (last 100): -2.140\n",
            "Episode 70200 Average Reward (last 100): -0.770\n",
            "Episode 70300 Average Reward (last 100): -2.350\n",
            "Episode 70400 Average Reward (last 100): -4.770\n",
            "Episode 70500 Average Reward (last 100): -4.940\n",
            "Episode 70600 Average Reward (last 100): -3.370\n",
            "Episode 70700 Average Reward (last 100): -3.600\n",
            "Episode 70800 Average Reward (last 100): -2.710\n",
            "Episode 70900 Average Reward (last 100): -4.120\n",
            "Episode 71000 Average Reward (last 100): -2.890\n",
            "Episode 71100 Average Reward (last 100): -1.800\n",
            "Episode 71200 Average Reward (last 100): -3.130\n",
            "Episode 71300 Average Reward (last 100): -2.930\n",
            "Episode 71400 Average Reward (last 100): 0.140\n",
            "Episode 71500 Average Reward (last 100): -3.530\n",
            "Episode 71600 Average Reward (last 100): -3.750\n",
            "Episode 71700 Average Reward (last 100): -1.880\n",
            "Episode 71800 Average Reward (last 100): -1.670\n",
            "Episode 71900 Average Reward (last 100): -2.340\n",
            "Episode 72000 Average Reward (last 100): -3.160\n",
            "Episode 72100 Average Reward (last 100): -1.420\n",
            "Episode 72200 Average Reward (last 100): -2.250\n",
            "Episode 72300 Average Reward (last 100): -1.330\n",
            "Episode 72400 Average Reward (last 100): -5.260\n",
            "Episode 72500 Average Reward (last 100): -4.620\n",
            "Episode 72600 Average Reward (last 100): -3.800\n",
            "Episode 72700 Average Reward (last 100): -3.280\n",
            "Episode 72800 Average Reward (last 100): -2.170\n",
            "Episode 72900 Average Reward (last 100): -2.160\n",
            "Episode 73000 Average Reward (last 100): -3.640\n",
            "Episode 73100 Average Reward (last 100): -3.360\n",
            "Episode 73200 Average Reward (last 100): -3.970\n",
            "Episode 73300 Average Reward (last 100): -0.860\n",
            "Episode 73400 Average Reward (last 100): -3.820\n",
            "Episode 73500 Average Reward (last 100): -3.190\n",
            "Episode 73600 Average Reward (last 100): -2.160\n",
            "Episode 73700 Average Reward (last 100): -3.160\n",
            "Episode 73800 Average Reward (last 100): -3.600\n",
            "Episode 73900 Average Reward (last 100): -2.410\n",
            "Episode 74000 Average Reward (last 100): -3.250\n",
            "Episode 74100 Average Reward (last 100): -3.720\n",
            "Episode 74200 Average Reward (last 100): -3.350\n",
            "Episode 74300 Average Reward (last 100): -5.450\n",
            "Episode 74400 Average Reward (last 100): -2.250\n",
            "Episode 74500 Average Reward (last 100): -1.040\n",
            "Episode 74600 Average Reward (last 100): -2.910\n",
            "Episode 74700 Average Reward (last 100): -2.620\n",
            "Episode 74800 Average Reward (last 100): -5.440\n",
            "Episode 74900 Average Reward (last 100): -1.660\n",
            "Episode 75000 Average Reward (last 100): -2.920\n",
            "Episode 75100 Average Reward (last 100): -2.810\n",
            "Episode 75200 Average Reward (last 100): -2.000\n",
            "Episode 75300 Average Reward (last 100): -3.420\n",
            "Episode 75400 Average Reward (last 100): -3.960\n",
            "Episode 75500 Average Reward (last 100): -5.200\n",
            "Episode 75600 Average Reward (last 100): -4.630\n",
            "Episode 75700 Average Reward (last 100): -3.630\n",
            "Episode 75800 Average Reward (last 100): -2.540\n",
            "Episode 75900 Average Reward (last 100): -4.280\n",
            "Episode 76000 Average Reward (last 100): -3.400\n",
            "Episode 76100 Average Reward (last 100): -2.930\n",
            "Episode 76200 Average Reward (last 100): -3.620\n",
            "Episode 76300 Average Reward (last 100): -2.970\n",
            "Episode 76400 Average Reward (last 100): -1.810\n",
            "Episode 76500 Average Reward (last 100): -4.650\n",
            "Episode 76600 Average Reward (last 100): -3.510\n",
            "Episode 76700 Average Reward (last 100): -4.610\n",
            "Episode 76800 Average Reward (last 100): -3.770\n",
            "Episode 76900 Average Reward (last 100): -4.710\n",
            "Episode 77000 Average Reward (last 100): -2.490\n",
            "Episode 77100 Average Reward (last 100): -2.810\n",
            "Episode 77200 Average Reward (last 100): -3.660\n",
            "Episode 77300 Average Reward (last 100): -3.170\n",
            "Episode 77400 Average Reward (last 100): -2.550\n",
            "Episode 77500 Average Reward (last 100): -3.090\n",
            "Episode 77600 Average Reward (last 100): -3.540\n",
            "Episode 77700 Average Reward (last 100): -3.960\n",
            "Episode 77800 Average Reward (last 100): -2.880\n",
            "Episode 77900 Average Reward (last 100): -3.140\n",
            "Episode 78000 Average Reward (last 100): -2.430\n",
            "Episode 78100 Average Reward (last 100): -3.060\n",
            "Episode 78200 Average Reward (last 100): -2.160\n",
            "Episode 78300 Average Reward (last 100): -1.150\n",
            "Episode 78400 Average Reward (last 100): -2.310\n",
            "Episode 78500 Average Reward (last 100): -3.310\n",
            "Episode 78600 Average Reward (last 100): -3.240\n",
            "Episode 78700 Average Reward (last 100): -2.650\n",
            "Episode 78800 Average Reward (last 100): -4.700\n",
            "Episode 78900 Average Reward (last 100): -2.890\n",
            "Episode 79000 Average Reward (last 100): -2.720\n",
            "Episode 79100 Average Reward (last 100): -3.370\n",
            "Episode 79200 Average Reward (last 100): -3.890\n",
            "Episode 79300 Average Reward (last 100): -2.950\n",
            "Episode 79400 Average Reward (last 100): -2.260\n",
            "Episode 79500 Average Reward (last 100): -1.730\n",
            "Episode 79600 Average Reward (last 100): -4.120\n",
            "Episode 79700 Average Reward (last 100): -2.030\n",
            "Episode 79800 Average Reward (last 100): -1.440\n",
            "Episode 79900 Average Reward (last 100): -3.880\n",
            "Episode 80000 Average Reward (last 100): -2.370\n",
            "Episode 80100 Average Reward (last 100): -1.940\n",
            "Episode 80200 Average Reward (last 100): -3.660\n",
            "Episode 80300 Average Reward (last 100): -1.350\n",
            "Episode 80400 Average Reward (last 100): -4.770\n",
            "Episode 80500 Average Reward (last 100): -2.940\n",
            "Episode 80600 Average Reward (last 100): -5.340\n",
            "Episode 80700 Average Reward (last 100): -2.490\n",
            "Episode 80800 Average Reward (last 100): -2.700\n",
            "Episode 80900 Average Reward (last 100): -3.010\n",
            "Episode 81000 Average Reward (last 100): -3.720\n",
            "Episode 81100 Average Reward (last 100): -1.910\n",
            "Episode 81200 Average Reward (last 100): -3.730\n",
            "Episode 81300 Average Reward (last 100): -4.560\n",
            "Episode 81400 Average Reward (last 100): -3.260\n",
            "Episode 81500 Average Reward (last 100): -2.930\n",
            "Episode 81600 Average Reward (last 100): -1.920\n",
            "Episode 81700 Average Reward (last 100): -3.310\n",
            "Episode 81800 Average Reward (last 100): -2.900\n",
            "Episode 81900 Average Reward (last 100): -4.830\n",
            "Episode 82000 Average Reward (last 100): -2.830\n",
            "Episode 82100 Average Reward (last 100): -3.380\n",
            "Episode 82200 Average Reward (last 100): -3.450\n",
            "Episode 82300 Average Reward (last 100): -3.220\n",
            "Episode 82400 Average Reward (last 100): -3.850\n",
            "Episode 82500 Average Reward (last 100): -3.110\n",
            "Episode 82600 Average Reward (last 100): -3.150\n",
            "Episode 82700 Average Reward (last 100): -4.700\n",
            "Episode 82800 Average Reward (last 100): -3.940\n",
            "Episode 82900 Average Reward (last 100): -1.320\n",
            "Episode 83000 Average Reward (last 100): -1.910\n",
            "Episode 83100 Average Reward (last 100): -2.140\n",
            "Episode 83200 Average Reward (last 100): -4.100\n",
            "Episode 83300 Average Reward (last 100): -2.970\n",
            "Episode 83400 Average Reward (last 100): -4.910\n",
            "Episode 83500 Average Reward (last 100): -2.220\n",
            "Episode 83600 Average Reward (last 100): -2.150\n",
            "Episode 83700 Average Reward (last 100): -4.060\n",
            "Episode 83800 Average Reward (last 100): -1.880\n",
            "Episode 83900 Average Reward (last 100): -3.320\n",
            "Episode 84000 Average Reward (last 100): -3.000\n",
            "Episode 84100 Average Reward (last 100): -3.340\n",
            "Episode 84200 Average Reward (last 100): -2.570\n",
            "Episode 84300 Average Reward (last 100): -2.070\n",
            "Episode 84400 Average Reward (last 100): -4.880\n",
            "Episode 84500 Average Reward (last 100): -2.740\n",
            "Episode 84600 Average Reward (last 100): -2.790\n",
            "Episode 84700 Average Reward (last 100): -4.140\n",
            "Episode 84800 Average Reward (last 100): -2.350\n",
            "Episode 84900 Average Reward (last 100): -4.180\n",
            "Episode 85000 Average Reward (last 100): -2.730\n",
            "Episode 85100 Average Reward (last 100): -4.070\n",
            "Episode 85200 Average Reward (last 100): -4.310\n",
            "Episode 85300 Average Reward (last 100): -3.020\n",
            "Episode 85400 Average Reward (last 100): -2.640\n",
            "Episode 85500 Average Reward (last 100): -3.650\n",
            "Episode 85600 Average Reward (last 100): -1.980\n",
            "Episode 85700 Average Reward (last 100): -1.920\n",
            "Episode 85800 Average Reward (last 100): -3.540\n",
            "Episode 85900 Average Reward (last 100): -3.140\n",
            "Episode 86000 Average Reward (last 100): -2.590\n",
            "Episode 86100 Average Reward (last 100): -4.520\n",
            "Episode 86200 Average Reward (last 100): -2.870\n",
            "Episode 86300 Average Reward (last 100): -1.700\n",
            "Episode 86400 Average Reward (last 100): -1.230\n",
            "Episode 86500 Average Reward (last 100): -3.970\n",
            "Episode 86600 Average Reward (last 100): -2.090\n",
            "Episode 86700 Average Reward (last 100): -2.220\n",
            "Episode 86800 Average Reward (last 100): -1.980\n",
            "Episode 86900 Average Reward (last 100): -1.910\n",
            "Episode 87000 Average Reward (last 100): -3.160\n",
            "Episode 87100 Average Reward (last 100): -2.820\n",
            "Episode 87200 Average Reward (last 100): -3.310\n",
            "Episode 87300 Average Reward (last 100): -4.250\n",
            "Episode 87400 Average Reward (last 100): -2.580\n",
            "Episode 87500 Average Reward (last 100): -2.670\n",
            "Episode 87600 Average Reward (last 100): -4.270\n",
            "Episode 87700 Average Reward (last 100): -4.320\n",
            "Episode 87800 Average Reward (last 100): -4.200\n",
            "Episode 87900 Average Reward (last 100): -2.230\n",
            "Episode 88000 Average Reward (last 100): -3.190\n",
            "Episode 88100 Average Reward (last 100): -5.740\n",
            "Episode 88200 Average Reward (last 100): -2.670\n",
            "Episode 88300 Average Reward (last 100): -3.370\n",
            "Episode 88400 Average Reward (last 100): -3.220\n",
            "Episode 88500 Average Reward (last 100): -1.550\n",
            "Episode 88600 Average Reward (last 100): -4.510\n",
            "Episode 88700 Average Reward (last 100): -1.950\n",
            "Episode 88800 Average Reward (last 100): -2.810\n",
            "Episode 88900 Average Reward (last 100): -1.400\n",
            "Episode 89000 Average Reward (last 100): -3.820\n",
            "Episode 89100 Average Reward (last 100): -4.570\n",
            "Episode 89200 Average Reward (last 100): -5.750\n",
            "Episode 89300 Average Reward (last 100): -2.770\n",
            "Episode 89400 Average Reward (last 100): -3.510\n",
            "Episode 89500 Average Reward (last 100): -1.710\n",
            "Episode 89600 Average Reward (last 100): -3.620\n",
            "Episode 89700 Average Reward (last 100): -2.880\n",
            "Episode 89800 Average Reward (last 100): -3.250\n",
            "Episode 89900 Average Reward (last 100): -2.840\n",
            "Episode 90000 Average Reward (last 100): -3.590\n",
            "Episode 90100 Average Reward (last 100): -2.350\n",
            "Episode 90200 Average Reward (last 100): -2.870\n",
            "Episode 90300 Average Reward (last 100): -3.040\n",
            "Episode 90400 Average Reward (last 100): -2.960\n",
            "Episode 90500 Average Reward (last 100): -5.990\n",
            "Episode 90600 Average Reward (last 100): -3.470\n",
            "Episode 90700 Average Reward (last 100): -1.620\n",
            "Episode 90800 Average Reward (last 100): -1.720\n",
            "Episode 90900 Average Reward (last 100): -2.100\n",
            "Episode 91000 Average Reward (last 100): -4.300\n",
            "Episode 91100 Average Reward (last 100): -4.580\n",
            "Episode 91200 Average Reward (last 100): -2.560\n",
            "Episode 91300 Average Reward (last 100): -3.980\n",
            "Episode 91400 Average Reward (last 100): -2.960\n",
            "Episode 91500 Average Reward (last 100): -3.490\n",
            "Episode 91600 Average Reward (last 100): -3.960\n",
            "Episode 91700 Average Reward (last 100): -2.110\n",
            "Episode 91800 Average Reward (last 100): -2.500\n",
            "Episode 91900 Average Reward (last 100): -1.240\n",
            "Episode 92000 Average Reward (last 100): -3.450\n",
            "Episode 92100 Average Reward (last 100): -3.070\n",
            "Episode 92200 Average Reward (last 100): -3.260\n",
            "Episode 92300 Average Reward (last 100): -0.960\n",
            "Episode 92400 Average Reward (last 100): -3.780\n",
            "Episode 92500 Average Reward (last 100): -3.190\n",
            "Episode 92600 Average Reward (last 100): -4.820\n",
            "Episode 92700 Average Reward (last 100): -2.320\n",
            "Episode 92800 Average Reward (last 100): -1.730\n",
            "Episode 92900 Average Reward (last 100): -0.980\n",
            "Episode 93000 Average Reward (last 100): -2.100\n",
            "Episode 93100 Average Reward (last 100): -2.330\n",
            "Episode 93200 Average Reward (last 100): -2.230\n",
            "Episode 93300 Average Reward (last 100): -4.110\n",
            "Episode 93400 Average Reward (last 100): -3.260\n",
            "Episode 93500 Average Reward (last 100): -2.930\n",
            "Episode 93600 Average Reward (last 100): -3.280\n",
            "Episode 93700 Average Reward (last 100): -4.580\n",
            "Episode 93800 Average Reward (last 100): -2.840\n",
            "Episode 93900 Average Reward (last 100): -5.520\n",
            "Episode 94000 Average Reward (last 100): -2.010\n",
            "Episode 94100 Average Reward (last 100): -1.800\n",
            "Episode 94200 Average Reward (last 100): -1.420\n",
            "Episode 94300 Average Reward (last 100): -2.660\n",
            "Episode 94400 Average Reward (last 100): -1.920\n",
            "Episode 94500 Average Reward (last 100): -4.180\n",
            "Episode 94600 Average Reward (last 100): -4.110\n",
            "Episode 94700 Average Reward (last 100): -1.190\n",
            "Episode 94800 Average Reward (last 100): -2.170\n",
            "Episode 94900 Average Reward (last 100): -1.650\n",
            "Episode 95000 Average Reward (last 100): -2.490\n",
            "Episode 95100 Average Reward (last 100): -3.220\n",
            "Episode 95200 Average Reward (last 100): -3.760\n",
            "Episode 95300 Average Reward (last 100): -2.500\n",
            "Episode 95400 Average Reward (last 100): -3.610\n",
            "Episode 95500 Average Reward (last 100): -1.980\n",
            "Episode 95600 Average Reward (last 100): -1.030\n",
            "Episode 95700 Average Reward (last 100): -3.240\n",
            "Episode 95800 Average Reward (last 100): -2.230\n",
            "Episode 95900 Average Reward (last 100): -2.400\n",
            "Episode 96000 Average Reward (last 100): -3.110\n",
            "Episode 96100 Average Reward (last 100): -3.680\n",
            "Episode 96200 Average Reward (last 100): -2.700\n",
            "Episode 96300 Average Reward (last 100): -1.130\n",
            "Episode 96400 Average Reward (last 100): -3.790\n",
            "Episode 96500 Average Reward (last 100): -2.290\n",
            "Episode 96600 Average Reward (last 100): -2.060\n",
            "Episode 96700 Average Reward (last 100): -4.080\n",
            "Episode 96800 Average Reward (last 100): -4.740\n",
            "Episode 96900 Average Reward (last 100): -1.770\n",
            "Episode 97000 Average Reward (last 100): -2.250\n",
            "Episode 97100 Average Reward (last 100): -0.990\n",
            "Episode 97200 Average Reward (last 100): -4.180\n",
            "Episode 97300 Average Reward (last 100): -4.300\n",
            "Episode 97400 Average Reward (last 100): -1.520\n",
            "Episode 97500 Average Reward (last 100): -3.590\n",
            "Episode 97600 Average Reward (last 100): -2.210\n",
            "Episode 97700 Average Reward (last 100): -2.080\n",
            "Episode 97800 Average Reward (last 100): -3.400\n",
            "Episode 97900 Average Reward (last 100): -2.820\n",
            "Episode 98000 Average Reward (last 100): -3.500\n",
            "Episode 98100 Average Reward (last 100): -1.750\n",
            "Episode 98200 Average Reward (last 100): -3.430\n",
            "Episode 98300 Average Reward (last 100): -3.770\n",
            "Episode 98400 Average Reward (last 100): -3.730\n",
            "Episode 98500 Average Reward (last 100): -3.770\n",
            "Episode 98600 Average Reward (last 100): -2.650\n",
            "Episode 98700 Average Reward (last 100): -2.940\n",
            "Episode 98800 Average Reward (last 100): -4.430\n",
            "Episode 98900 Average Reward (last 100): -1.220\n",
            "Episode 99000 Average Reward (last 100): -2.520\n",
            "Episode 99100 Average Reward (last 100): -2.280\n",
            "Episode 99200 Average Reward (last 100): -2.540\n",
            "Episode 99300 Average Reward (last 100): -3.400\n",
            "Episode 99400 Average Reward (last 100): -4.320\n",
            "Episode 99500 Average Reward (last 100): -2.710\n",
            "Episode 99600 Average Reward (last 100): -2.580\n",
            "Episode 99700 Average Reward (last 100): -1.640\n",
            "Episode 99800 Average Reward (last 100): -2.370\n",
            "Episode 99900 Average Reward (last 100): -3.290\n",
            "Episode 100000 Average Reward (last 100): -1.610\n",
            "Últimos resultados: media = -2.0 , desvio padrao = 8.549853799919621\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'plot_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32md:\\Projects\\Taia\\Monte-Carlo-Off-Policy\\Projeto_Reforço.ipynb Célula: 42\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Taia/Monte-Carlo-Off-Policy/Projeto_Refor%C3%A7o.ipynb#X56sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Mostra um gráfico de episódios x retornos (não descontados)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Taia/Monte-Carlo-Off-Policy/Projeto_Refor%C3%A7o.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Taia/Monte-Carlo-Off-Policy/Projeto_Refor%C3%A7o.ipynb#X56sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/montecarloOnP-\u001b[39m\u001b[39m{\u001b[39;00mENV_NAME\u001b[39m.\u001b[39mlower()[\u001b[39m0\u001b[39m:\u001b[39m8\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m-ep\u001b[39m\u001b[39m{\u001b[39;00mEPISODES\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/Taia/Monte-Carlo-Off-Policy/Projeto_Refor%C3%A7o.ipynb#X56sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m plot_result(rewards, r_max_plot,\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monPolicyD\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Taia/Monte-Carlo-Off-Policy/Projeto_Refor%C3%A7o.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# test_greedy_Q_policy(env, Qtable, 10, True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Taia/Monte-Carlo-Off-Policy/Projeto_Refor%C3%A7o.ipynb#X56sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'plot_result' is not defined"
          ]
        }
      ],
      "source": [
        "ENV_NAME = \"Taxi-v3\"  \n",
        "#ENV_NAME = \"MountainCarContinuous-v0\"  \n",
        "#ENV_NAME = \"LunarLander-v2\"  \n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "#parameters: {'learning_rate': 0.007617314337157292, 'gamma': 0.8762195844017038, 'epsilon': 0.1594319424288697}. Best is trial 16 with value: -2.2.\n",
        "if __name__ == \"__main__\":\n",
        "    r_max_plot = 10\n",
        "\n",
        "    EPISODES = 100000\n",
        "    LR = 0.007617314337157292\n",
        "    GAMMA = 0.8762195844017038\n",
        "    EPSILON = 0.1594319424288697\n",
        "\n",
        "    \n",
        "    # Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "    rewards, Qtable = run_montecarloOnP(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "    # Mostra um gráfico de episódios x retornos (não descontados)\n",
        "    # Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "    filename = f\"results/montecarloOnP-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "    plot_result(rewards, r_max_plot,100, 'onPolicyD')\n",
        "\n",
        "    # test_greedy_Q_policy(env, Qtable, 10, True)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lch6U12-WD3"
      },
      "source": [
        "## Contínuo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9kB46gwn-Yra"
      },
      "outputs": [],
      "source": [
        "\n",
        "ENV_NAME = \"MountainCar-v0\"  \n",
        "\n",
        "env = gym.make(ENV_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TEiYsqr3P9mU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GeneralDiscretizer:\n",
        "    def __init__(self, env, bins_per_dimension):\n",
        "        self.bins_per_dim = bins_per_dimension.copy()\n",
        "        self.intervals_per_dim = []\n",
        "        self.total_bins = 1\n",
        "        for i, bins in enumerate(bins_per_dimension):\n",
        "            self.intervals_per_dim.append(\n",
        "                np.linspace(env.observation_space.low[i], env.observation_space.high[i], bins+1) )\n",
        "            self.total_bins *= bins\n",
        "\n",
        "    def to_single_bin(self, state):\n",
        "        bin_vector = [(np.digitize(x=state[i], bins=intervals) - 1)\n",
        "                      for i, intervals in enumerate(self.intervals_per_dim)]\n",
        "        # print(bin_vector)\n",
        "        return self._bin_vector_to_single_bin(bin_vector, len(bin_vector)-1)\n",
        "\n",
        "    def _bin_vector_to_single_bin(self, vector, index):\n",
        "        if index < 0:\n",
        "            return 0\n",
        "        return vector[index] + self.bins_per_dim[index] * self._bin_vector_to_single_bin(vector, index-1)\n",
        "\n",
        "    def get_total_bins(self):\n",
        "        return self.total_bins\n",
        "\n",
        "\n",
        "class DiscreteObservationWrapper(gym.ObservationWrapper):\n",
        "    '''Classe para converter espaços contínuos em espaços discretos.\n",
        "    Esta classe converte ambientes de observações (estados) contínuos em ambientes de estados\n",
        "    discretos. Especificamente, ele converte representações dadas na forma de array de valores float\n",
        "    em um único inteiro $\\geq$ não-negativo (>=0).\n",
        "    \n",
        "    Precisa passar para o construtor uma lista que informa em quantos \"bins\" vai ser discretizada \n",
        "    cada dimensão (ou seja, cada valor float) do espaço de estados original.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, env, BINS_PER_DIMENSION):\n",
        "        super().__init__(env)\n",
        "        # cria um GeneralDiscretizer para converter um array de valores float em um único inteiro >= 0\n",
        "        # precisa dizer em quantos \"bins\" vai ser discretizada cada dimensão\n",
        "        self.discretizer = GeneralDiscretizer(env, BINS_PER_DIMENSION)\n",
        "        self.observation_space = gym.spaces.Discrete(self.discretizer.get_total_bins())\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return self.discretizer.to_single_bin(obs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st7lYLp_P9mV"
      },
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import gamma\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "\n",
        "# Esta função faz um treinamento com o Expected-SARSA, usando parâmetros sugeridos pelo Optuna.\n",
        "# Retorna a média dos retornos dos últimos 100 episódios.\n",
        "def train_valuesOf(trial : optuna.Trial):\n",
        "    \n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    gamma = trial.suggest_uniform('gamma', 0.02, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    bins1 = trial.suggest_int('bins1', 5.0, 80.0)\n",
        "    bins2 = trial.suggest_int('bins2', 10.0, 90.0)\n",
        "    \n",
        "    \n",
        "    print(f\"\\nTRIAL #{trial.number}: eps={eps}, gamma={gamma}, bins1={bins1},bins2={bins2}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    env_wrapper = DiscreteObservationWrapper(env, [bins1,bins2])\n",
        "    (returns, _) = run_montecarloOffP(env_wrapper, 20000, gamma,eps, render=False)\n",
        "    return sum(returns[-100:])/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfPnGz-NP9mW"
      },
      "outputs": [],
      "source": [
        "def train_valuesOn(trial : optuna.Trial):\n",
        "    \n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    gamma = trial.suggest_uniform('gamma', 0.02, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    bins1 = trial.suggest_int('bins1', 5.0, 80.0)\n",
        "    bins2 = trial.suggest_int('bins2', 10.0, 90.0)\n",
        "   \n",
        "    \n",
        "    print(f\"\\nTRIAL #{trial.number}: eps={eps}, gamma={gamma}, bins1={bins1},bins2={bins2}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    env_wrapper = DiscreteObservationWrapper(env, [bins1,bins2])\n",
        "    (returns, _) = run_montecarloOnP(env_wrapper, 20000, gamma, eps, render=False)\n",
        "    return sum(returns[-100:])/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0MAejiOxP9mW",
        "outputId": "033aa398-d01a-416d-c698-0d75327ec4af"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize', \n",
        "                            storage='sqlite:///optuna_studies.db', \n",
        "                            study_name= 'new_MC_offpolice_cont_bins_on-Policy', \n",
        "                            load_if_exists=True)\n",
        "\n",
        "\n",
        "study2 = optuna.create_study(direction='maximize', \n",
        "                            storage='sqlite:///optuna_studies.db', \n",
        "                            study_name= 'new_MC_offpolice_cont_bins_off-policy', \n",
        "                            load_if_exists=True)\n",
        "\n",
        "\n",
        "study.optimize(train_valuesOn, n_trials=30) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vZzK8JBKpim"
      },
      "outputs": [],
      "source": [
        "study2.optimize(train_valuesOf, n_trials=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aio19xcn-cUA",
        "outputId": "19301a35-2801-4fae-ff3a-4e5828ec03a5"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    r_max_plot = 10\n",
        "\n",
        "    EPISODES = 100000\n",
        "    GAMMA = 0.830525147061507\n",
        "    EPSILON = 0.05919712699520377\n",
        "\n",
        "    print(\"Ambiente Contínuo\")\n",
        "    # Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "    print(\"Monte-Carlo On-Policy\")\n",
        "    env_wrapper = DiscreteObservationWrapper(env, [70,70])\n",
        "    rewardsOn, QtableOn = run_montecarloOnP(env_wrapper, EPISODES, GAMMA, EPSILON, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "    filename = f\"results/montecarloOnPolicyCont-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "    plot_result(rewards, r_max_plot,100, 'onPolicyC')\n",
        "\n",
        "    #test_greedy_Q_policy(env, QtableOn, 10, True)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCljnzSZcw4F",
        "outputId": "a917656d-eb94-42bd-ba1a-8eff48246f80"
      },
      "outputs": [],
      "source": [
        "    env_wrapper = DiscreteObservationWrapper(env, [70,70])\n",
        "    print(\"Monte-Carlo Off-Policy\")\n",
        "    env_wrapper = DiscreteObservationWrapper(env, [70,70])\n",
        "    rewardsOff, QtableOff = run_montecarloOffP(env_wrapper, EPISODES, GAMMA, EPSILON, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "    # Mostra um gráfico de episódios x retornos (não descontados)\n",
        "    # Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "    filename = f\"results/montecarloOffPolicyCont-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "    plot_result(rewards, r_max_plot,100, 'offPolicyC')\n",
        "\n",
        "    #test_greedy_Q_policy(env, QtableOff, 10, True)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH49RrTEawin"
      },
      "source": [
        "Continuo Ambiente 2 ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIZlQQ7t1OPH",
        "outputId": "2fa9ccae-f15e-4818-f7e2-b22fc186abf8"
      },
      "outputs": [],
      "source": [
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xdYD8ayHdezv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Espaço de estados/observações:  Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
            "  - formato:  (3,)\n",
            "  - exemplo:  [ 0.90833104 -0.41825196  0.8534491 ]\n"
          ]
        }
      ],
      "source": [
        "# ENV_NAME = \"CarRacing-v1\"  \n",
        "\n",
        "# env = gym.make(ENV_NAME)\n",
        "env = gym.make(\"Pendulum-v1\")\n",
        "\n",
        "# vamos ver como é um estado deste ambiente?\n",
        "print(\"Espaço de estados/observações: \", env.observation_space)\n",
        "print(\"  - formato: \", env.observation_space.shape)\n",
        "print(\"  - exemplo: \", env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4aEhXCYfFuI"
      },
      "outputs": [],
      "source": [
        "def train_valuesOnCar(trial : optuna.Trial):\n",
        "    \n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    gamma = trial.suggest_uniform('gamma', 0.02, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    # bins1 = trial.suggest_int('bins1', -1, 1)\n",
        "    bins2 = trial.suggest_int('bins2', 5.0, 30.0)\n",
        "    bins3 = trial.suggest_int('bins3', 5.0, 10.0)\n",
        "   \n",
        "    \n",
        "    print(f\"\\nTRIAL #{trial.number}: eps={eps}, gamma={gamma},bins2={bins2},bins3={bins3}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    env_wrapper = DiscreteObservationWrapper(env, [bins2,bins3])\n",
        "    (returns, _) = run_montecarloOffP(env_wrapper, 2000, gamma, eps, render=False)\n",
        "    return sum(returns[-100:])/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "LUiKZv05xFBe",
        "outputId": "09e26183-66bd-49ce-dc4c-a375cac5baca"
      },
      "outputs": [],
      "source": [
        "\n",
        "study = optuna.create_study(direction='maximize', \n",
        "                            storage='sqlite:///optuna_studies.db', \n",
        "                            study_name= 'new_MC_offpolice_cont_bins_off-policy', \n",
        "                            load_if_exists=True)\n",
        "\n",
        "study.optimize(train_valuesOnCar, n_trials=10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "r_BcvZAi1zgv",
        "outputId": "51ddd1f3-fe97-4ac7-a64e-7c415deb7c32"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    r_max_plot = 10\n",
        "\n",
        "    EPISODES = 100000\n",
        "    LR = 0.01\n",
        "    GAMMA = 0.830525147061507\n",
        "    EPSILON = 0.05919712699520377\n",
        " \n",
        "    #env_wrapper = DiscreteObservationWrapper(env, [1,80,90])\n",
        "\n",
        "    env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "    \n",
        "    # Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "    rewards, Qtable = run_montecarloOffP(env, EPISODES, GAMMA, EPSILON, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "    # Mostra um gráfico de episódios x retornos (não descontados)\n",
        "    # Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "    filename = f\"results/montecarloOffPCar-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "    plot_result(rewards, r_max_plot,100, 'offPolicyC-CarRacing')\n",
        "\n",
        "    # test_greedy_Q_policy(env, Qtable, 10, True)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm18G8Wc5gZR",
        "outputId": "96ab212d-7193-4781-b591-922074b05e04"
      },
      "outputs": [],
      "source": [
        "rewards, Qtable = run_montecarloOnP(env, EPISODES, GAMMA, EPSILON, render=False)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "filename = f\"results/montecarloOffPCar-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards, r_max_plot,100, 'onPolicyC-CarRacing')\n",
        "\n",
        "   \n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpp_6Bzk0rRC"
      },
      "source": [
        "# Plot Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExH-n_L0xjh"
      },
      "outputs": [],
      "source": [
        "def test_greedy_Q_policy(env, Q, num_episodes=100, render=False, render_wait=0.01, video=None):\n",
        "    \"\"\"\n",
        "    Avalia a política gulosa (greedy) definida implicitamente por uma Q-table.\n",
        "    Ou seja, executa, em todo estado s, a ação \"a = argmax Q(s,a)\".\n",
        "    - env: o ambiente\n",
        "    - Q: a Q-table (tabela Q) que será usada\n",
        "    - num_episodes: quantidade de episódios a serem executados\n",
        "    - render: defina como True se deseja chamar `env.render()` a cada passo\n",
        "    - render_wait: intervalo de tempo entre as chamadas a `env.render()`\n",
        "    - video \n",
        "    \n",
        "    Retorna:\n",
        "    - um par contendo o valor escalar do retorno médio por episódio e \n",
        "       a lista de retornos de todos os episódios\n",
        "    \"\"\"\n",
        "    episode_returns = []\n",
        "    total_steps = 0\n",
        "    for i in range(num_episodes):\n",
        "        print(f\"Episode {i+1}\")\n",
        "        obs = env.reset()\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(render_wait)\n",
        "        if video is not None:\n",
        "            video.capture_frame()\n",
        "        done = False\n",
        "        episode_returns.append(0.0)\n",
        "        while not done:\n",
        "            action = np.argmax(Q[obs])\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            if render:\n",
        "                env.render()\n",
        "                time.sleep(render_wait)\n",
        "            if video is not None:\n",
        "                video.capture_frame()\n",
        "            total_steps += 1\n",
        "            episode_returns[-1] += reward\n",
        "        print(\"- retorno:\", episode_returns[-1])\n",
        "    mean_return = round(np.mean(episode_returns), 1)\n",
        "    print(\"Retorno médio (por episódio):\", mean_return, end=\"\")\n",
        "    print(\", episódios:\", len(episode_returns), end=\"\")\n",
        "    print(\", total de passos:\", total_steps)\n",
        "    show_state(env,total_steps)\n",
        "    if video is not None:\n",
        "        video.close()\n",
        "    return mean_return, episode_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nrj0SU4096s"
      },
      "outputs": [],
      "source": [
        "def smooth(data, window):\n",
        "  data = np.array(data)\n",
        "  n = len(data)\n",
        "  y = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    start = max(0, i-window+1)\n",
        "    y[i] = data[start:(i+1)].mean()\n",
        "  return y\n",
        "\n",
        "def plot_result(returns, ymax_suggested=None, window=100, filename=None):\n",
        "    '''Exibe um gráfico \"retornos x recompensas\", fazendo a média a cada 100 retornos, para suavizar.     \n",
        "    Se o parâmetro filename for fornecido, salva o gráfico em arquivo ao invés de exibir.\n",
        "    \n",
        "    Parâmetros:\n",
        "    - returns: lista de retornos a cada episódio\n",
        "    - ymax_suggested (opcional): valor máximo de retorno (eixo y), se tiver um valor máximo conhecido previamente\n",
        "    - filename: indique um nome de arquivo, se quiser salvar a imagem do gráfico; senão, o gráfico será apenas exibido\n",
        "    '''\n",
        "    plt.figure(figsize=(14,8))\n",
        "    smoothed_returns = smooth(returns, window)\n",
        "    xvalues = np.arange(1, len(returns)+1)\n",
        "    plt.plot(xvalues, smoothed_returns)\n",
        "    plt.xlabel('Episódios')\n",
        "    plt.ylabel('Retorno')\n",
        "    if ymax_suggested is not None:\n",
        "        ymax = np.max([ymax_suggested, np.max(smoothed_returns)])\n",
        "        plt.ylim(top=ymax)\n",
        "    plt.title(f\"Retorno médio a cada {window} episódios\")\n",
        "    if filename is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(filename)\n",
        "        print(\"Arquivo salvo:\", filename)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lddRTK2LdWvI"
      },
      "source": [
        "# Exibe o ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUY2uDh-P9mX"
      },
      "outputs": [],
      "source": [
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEJxnEr5kPkS"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'MountainCar-v0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRspInThP9mX"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gktQMtqu_g7a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vai9dMYA_keu"
      },
      "outputs": [],
      "source": [
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "BMSFweJIsB9u",
        "outputId": "ad78e652-59f9-4606-c988-cb4a1e88958c"
      },
      "outputs": [],
      "source": [
        "#record_video(ENV_NAME, model, video_length=1000, prefix='monte-car-off-police')\n",
        "\n",
        "videoOf = VideoRecorder(env, \"monte-car-off-police.mp4\")\n",
        "#videoOn = VideoRecorder(env, \"monte-car-on-police.mp4\")\n",
        "#test_greedy_Q_policy(env, QtableOn, 10, True,videoOn)\n",
        "#render_mp4('video_offpolicy')\n",
        "\n",
        "test_greedy_Q_policy(env, QtableOn, 10, True,videoOf)\n",
        "render_mp4('monte-car-off-police.mp4')\n",
        "#show_videos('videos', prefix='monte-car-off-police')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CwJjLOm-qo5x",
        "_Usxq5Z1qwbj",
        "pLlWktrRj9GZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "de7c37f11048604427e696a7e966b00a255ebcd16cd0a44f6d549979884e8d96"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
